{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save a Notebook session:\n",
    "\n",
    "# import dill\n",
    "# dill.dump_session('notebook_env.db')\n",
    "\n",
    "# # Restore a Notebook session:\n",
    "\n",
    "# import dill\n",
    "# dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import igraph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from scipy.cluster import hierarchy\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for gsd files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "gsd_file = \"test_notebook/gel_l860_vfr0.3_vfp0_nG20_nR1170_nL390_k00_koff0.001_repuls500_bd1.0_Tc1.0_s1_dt0.002_gs0.001_N100000000.gsd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "prefix = '_'.join(gsd_file.split('_')[:-1])\n",
    "gsd_files = sorted( glob.glob(prefix + '_N*.gsd'),key=lambda x: x.split('_')[-1].replace('.gsd','').replace('N','') )\n",
    "print(gsd_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can probably speed this up later:\n",
    "def graph_distances(bonded_particles, particle_positions, box_size):\n",
    "    distances = np.zeros(len(bonded_particles))\n",
    "    for idx, pair in enumerate(bonded_particles):\n",
    "        i,j = pair\n",
    "        dr = particle_positions[j][1:4]-particle_positions[i][1:4]\n",
    "        dr = dr-box_size*np.floor(dr/box_size+0.5)\n",
    "        distances[idx] = np.sqrt((dr*dr).sum())\n",
    "    return distances\n",
    "\n",
    "#step time converts from MD steps to microseconds for this current setup\n",
    "def get_trajectory_graph_info(gsd_files,analysis_stride=1, step_time=7.5e-2):\n",
    "    from bond_analysis import bonds_analysis\n",
    "    from gsd import hoomd as gsd\n",
    "    trajectory = []\n",
    "    for gsd_file in gsd_files:\n",
    "        trajectory.extend(gsd.open(gsd_file,'rb')[1:]) # read gsd file\n",
    "    box_length = None\n",
    "    types = trajectory[0].particles.typeid\n",
    "    N_A = int((types==0).sum())\n",
    "    N_B = int((types==1).sum())\n",
    "    n_binders = N_A+N_B\n",
    "    assert N_A > N_B, \"Warning, these may not be the right labels for A (n=%i) and B (n=%i)\"%(N_A,N_B)\n",
    "        \n",
    "    time_list = []\n",
    "    graph_list = []\n",
    "    prev_positions = np.zeros((n_binders,3))\n",
    "    deltaT_seconds = step_time/1e6\n",
    "    for i in tqdm( range(0,len(trajectory),analysis_stride)):\n",
    "        #for debugging:\n",
    "        #if not i==100 and not i==101: continue\n",
    "        step = trajectory[i].configuration.step\n",
    "        box_size_i = trajectory[i].configuration.box[:3]\n",
    "        if box_length is None:\n",
    "            box_length = box_size_i[0]\n",
    "        else:\n",
    "            assert box_length == box_size_i[0], \"error, box size is changing\"\n",
    "\n",
    "        bonded_particles, particle_positions, diffusivity = bonds_analysis(trajectory,frame_id=i,step_time=step_time/1e6)\n",
    "\n",
    "        #skip frames before any bonding\n",
    "        if(len(bonded_particles)==0):\n",
    "            continue\n",
    "        step_time_seconds = step*deltaT_seconds\n",
    "        time_list.append(step_time_seconds)\n",
    "        #particle positions is index, x,y,z, diffusivity\n",
    "        g = igraph.Graph(n_binders, directed=False)\n",
    "        edges = bonded_particles\n",
    "        g.add_edges(edges)\n",
    "        g.es['length'] = graph_distances(bonded_particles, particle_positions, box_size_i)\n",
    "\n",
    "        g.vs['diffusivity'] = diffusivity\n",
    "        g.vs['coordinate'] = particle_positions[:n_binders,1:4]\n",
    "        graph_list.append(g)\n",
    "    df = pd.DataFrame({\n",
    "        'time': time_list,\n",
    "        'graph': graph_list\n",
    "    })\n",
    "    df.L = box_length\n",
    "    df.N_A = N_A\n",
    "    df.N_B = N_B\n",
    "    return df\n",
    " \n",
    "   \n",
    "df = get_trajectory_graph_info(gsd_files,analysis_stride=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the molecular concentration within random fixed volume changing with time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = df.L\n",
    "N_A = df.N_A\n",
    "N_B = df.N_B\n",
    "r_box = L/4\n",
    "r_center_list = np.array([[L/4,L/4,L/4],[-L/4,L/4,L/4],[L/4,-L/4,L/4],[-L/4,-L/4,L/4],[L/4,L/4,-L/4],[-L/4,L/4,-L/4],[L/4,-L/4,-L/4],[-L/4,-L/4,-L/4]]) #center of the investigate areas\n",
    "N_center = np.zeros(shape=(len(r_center_list),len(df)))\n",
    "concentrations = np.zeros(shape=(len(r_center_list),len(df)))\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    g = row['graph']\n",
    "    Coord = g.vs['coordinate']\n",
    "    for ridx, r_center in enumerate(r_center_list):\n",
    "        dr = Coord - r_center\n",
    "        n_in_box = ((np.abs(dr) < r_box).sum(axis=1)==3).sum()\n",
    "        N_center[ridx,index] = n_in_box\n",
    "        concentrations[ridx,index] = n_in_box*pow(10,7)/(pow(r_box*2,3)*6.02) #Calculate concentration within the cubic with uM unit\n",
    "\n",
    "average_concentration = len(Coord)*pow(10,7)/(pow(L,3)*6.02)\n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "for i in range(len(r_center_list)):\n",
    "    concentrations_i = concentrations[i,:]\n",
    "    ax.plot(df.time,concentrations_i,label=f'center at {r_center_list[i]}')\n",
    "ax.set_ylabel(f'Molecular concentration within cubic \\n with length L/2={L/2} nm (ÂµM)', fontsize=15)\n",
    "ax.set_xlabel('Time (s)', fontsize=15)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "ax.set_title(f'Dynamic of local molecular concentration')\n",
    "ax.axhline(average_concentration,linestyle='--',color='black')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import hierarchy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Here the first part of nodes are dimers and remainings are hexamers\n",
    "N_neighbor_time = []  # At each time points, number of neighbors of all nodes\n",
    "giant_node = []  # At each time points, node index within the largest cluster\n",
    "N_CONNECT_TOTAL = []  # Record number of nodes in the shortest path connecting two nodes in all time points\n",
    "ORDER = []  # Record nodes order in the rearranged cluster graphs\n",
    "\n",
    "plot_stride = 10\n",
    "for gidx, g in enumerate(tqdm(df.graph)):\n",
    "    N_neighbor = np.array([len(g.neighborhood(i)) - 1 for i in range(g.vcount())])\n",
    "    if N_neighbor.sum() == 0:\n",
    "        # skipping frames where there are no neighbors\n",
    "        continue\n",
    "    N_neighbor_time.append(N_neighbor)\n",
    "\n",
    "    Direct_neighbor = np.zeros((g.vcount(), g.vcount()), dtype=int)\n",
    "    for i in range(g.vcount()):\n",
    "        Direct_neighbor[i, g.neighborhood(i)[1:]] = 1\n",
    "\n",
    "    ccs = g.clusters()\n",
    "    ccslistsize = np.array(ccs.sizes())\n",
    "    giant_node.append(ccs[np.argmax(ccslistsize)])\n",
    "\n",
    "    # Calculate shortest paths length between nodes\n",
    "    spl_total = np.empty((g.vcount(), g.vcount()))\n",
    "    N_connect_total = np.empty_like(spl_total)\n",
    "    for v_source in range(g.vcount()):\n",
    "        spl = np.array(g.shortest_paths_dijkstra(v_source, g.vs(), weights=g.es['length'])).squeeze()\n",
    "        spl_total[v_source] = spl\n",
    "        N_connect = np.array([len(i) for i in g.get_shortest_paths(v_source, g.vs())])\n",
    "        N_connect_total[v_source] = N_connect\n",
    "\n",
    "    # Replace infinities with a very large distance\n",
    "    distances = spl_total.copy()\n",
    "    distances[np.isinf(distances)] = distances[~np.isinf(distances)].max()\n",
    "\n",
    "    # Clustering\n",
    "    threshold = 1\n",
    "\n",
    "    linkage = hierarchy.linkage(distance.squareform(distances), method=\"single\")\n",
    "    clusters = hierarchy.fcluster(linkage, threshold, criterion=\"distance\")\n",
    "    dend = hierarchy.dendrogram(linkage, color_threshold=threshold, no_plot=True)\n",
    "    order = np.array(dend['leaves'])\n",
    "    ORDER = np.append(ORDER, order, axis=0)\n",
    "\n",
    "    distances = distances[order][:, order]\n",
    "    N_connect_total = N_connect_total[order][:, order]\n",
    "    Direct_neighbor = Direct_neighbor[order][:, order]\n",
    "    if len(N_CONNECT_TOTAL) == 0:\n",
    "        N_CONNECT_TOTAL = N_connect_total\n",
    "    else:\n",
    "        N_CONNECT_TOTAL = np.append(N_CONNECT_TOTAL, N_connect_total, axis=0)\n",
    "\n",
    "    N_direct_neighbor = N_neighbor[order]\n",
    "    N_direct_neighbor_matrix = np.tile(N_direct_neighbor[:, None], (1, g.vcount()))\n",
    "    N_direct_neighbor_per = N_direct_neighbor / np.where(order < N_A, 2, 6)\n",
    "    N_label = np.where(order < N_A, 1, 0)\n",
    "    N_direct_neighbor_per_matrix = np.tile(N_direct_neighbor_per[:, None], (1, g.vcount()))\n",
    "    N_label_matrix = np.tile(N_label[:, None], (1, g.vcount()))\n",
    "\n",
    "#   Plot all the analysis results  \n",
    "    #After clustering, convert the largest distances (& not indirectly connected nodes) to nan and plot as white in the colormap\n",
    "    max_index = np.where(distances == np.amax(distances)) \n",
    "    distances[max_index] = np.nan\n",
    "    no_connect_index = np.where(N_connect_total == 0)\n",
    "    N_connect_total[no_connect_index] = np.nan\n",
    "    \n",
    "    if gidx%plot_stride == 0 or gidx==len(df.graph)-1:\n",
    "        #skip plotting for now, uncomment next line\n",
    "        #continue\n",
    "\n",
    "        fig, axs = plt.subplots(1,5,figsize=(35,6))\n",
    "\n",
    "        current_cmap = plt.cm.get_cmap()\n",
    "        current_cmap.set_bad(color='white')\n",
    "\n",
    "        [x,y] = np.where(Direct_neighbor == 1) #Extract x,y node position for those are direct neighbors\n",
    "\n",
    "        sc0 = axs[0].imshow(distances)\n",
    "    #     sc1 = axs[1].imshow(N_label_matrix)\n",
    "        sc1 = axs[1].scatter(x, y, s=0.1) \n",
    "        sc2 = axs[2].imshow(N_direct_neighbor_per_matrix)\n",
    "        sc3 = axs[3].imshow(N_connect_total)\n",
    "\n",
    "        axs[0].invert_yaxis()\n",
    "        axs[0].invert_xaxis()\n",
    "        axs[1].invert_xaxis()\n",
    "        axs[3].invert_yaxis()\n",
    "        axs[3].invert_xaxis()\n",
    "\n",
    "\n",
    "        cbar0 = fig.colorbar(sc0, ax=axs[0],extend='neither')\n",
    "    #     sc0.set_clim(vmin=0,vmax=1.1)\n",
    "\n",
    "        axs[1].set_xlim(0,Direct_neighbor.shape[0])\n",
    "        axs[1].set_ylim(0,Direct_neighbor.shape[0])\n",
    "        axs[1].set_aspect('equal')\n",
    "    #     cbar1 = fig.colorbar(sc1, ax=axs[1],ticks=[0, 1])\n",
    "    # #     cbar1.ax.set_yticklabels(['B', 'A'])\n",
    "    #     cbar1.ax.set_yticklabels(['N','Y'])\n",
    "\n",
    "        cbar2 = fig.colorbar(sc2, ax=axs[2],extend='neither')\n",
    "    #     sc2.set_clim(vmin=0,vmax=1)\n",
    "\n",
    "        cbar3 = fig.colorbar(sc3, ax=axs[3],extend='neither')\n",
    "    #     sc3.set_clim(vmin=0,vmax=53)\n",
    "\n",
    "        axs[2].xaxis.set_visible(False)\n",
    "        cbar0.set_label(r'Topological shortest distance (nm)', fontsize=18)\n",
    "    #     cbar1.set_label('Node type', fontsize=14)\n",
    "        axs[1].yaxis.set_label_position('right')\n",
    "        axs[1].set_ylabel('Direct Neighbors', fontsize=18)\n",
    "        cbar2.set_label('precentage of direct neighbors', fontsize=18)\n",
    "        cbar3.set_label('Number of nodes connecting path', fontsize=18)\n",
    "        axs[0].set_title(f't = {df.time[gidx]:.3f} s', fontsize=18)    \n",
    "        axs[3].set_title(f't = {df.time[gidx]:.3f} s', fontsize=18)    \n",
    "        axs[4].set_xlabel(\"Node\",fontsize=19)\n",
    "    #     axs[4].set_ylabel(\"Dissimilarity\")\n",
    "    #     plt.subplots_adjust(wspace = 0.3)\n",
    "    #    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate number of neighbors based on elements directly from graph or within largest cluster in each time frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the first part of nodes are dimers and remainings are hexamers\n",
    "N_neighbor_time = [] #At each time points, number of neighbors of all nodes\n",
    "giant_node = [] #At each time points, node index within the largest cluster\n",
    "N_CONNECT_TOTAL = [] #Record number of nodes in the shortest path connecting two nodes in all time points\n",
    "ORDER = [] #Record nodes order in the rearranged cluster graphs\n",
    "\n",
    "#for index, row in tqdm(df.tail(n=5).iterrows(), total=5):\n",
    "#for index, row in df.iterrows():#, total=df.shape[0]:\n",
    "    #g = row['graph']\n",
    "    #print(g)\n",
    "plot_stride=10\n",
    "for gidx, g in enumerate(tqdm(df.graph[::plot_stride])):\n",
    "    N_neighbor = [len(g.neighborhood(i))-1 for i in range(0,g.vcount())] #The first value in the g.neighborhood is the node index itself\n",
    "    if(np.sum(N_neighbor)==0): \n",
    "        #skipping frames where there are no neighbors\n",
    "        continue\n",
    "    N_neighbor_time.append(N_neighbor)\n",
    "    \n",
    "    Direct_neighbor = np.zeros((g.vcount(),g.vcount())) #Direct_neighbor[i,j] is 1 if (i,j) are direct neighbors\n",
    "    for i in range(0,g.vcount()):\n",
    "        Direct_neighbor[ i, g.neighborhood(i)[1:] ] = 1\n",
    "\n",
    "    ccs = g.clusters()\n",
    "    ccslistsize = list(ccs.sizes())\n",
    "    giant_node.append(ccs[ccslistsize.index(max(ccslistsize))])\n",
    "    \n",
    "    # Calculate shortest paths length between nodes\n",
    "    spl_total1 = []\n",
    "    N_connect_total = []\n",
    "    for v_source in range(0,g.vcount()):\n",
    "        spl = g.shortest_paths_dijkstra(v_source, g.vs(), weights=g.es['length'])\n",
    "        spl_total1.append(spl)\n",
    "        N_connect = [float(len(i)) for i in g.get_shortest_paths(v_source, g.vs())] #Number of nodes connecting v_source and v_target (including v_source and v_target)\n",
    "        N_connect_total.append(N_connect)\n",
    "    distances = np.array(spl_total1).reshape((g.vcount(),g.vcount()))\n",
    "    N_connect_total = np.array(N_connect_total).reshape((g.vcount(),g.vcount()))\n",
    "\n",
    "    # Replace infinities with a very large distance\n",
    "    distances[np.isinf(distances)] = distances[~np.isinf(distances)].max()\n",
    "    \n",
    "    # Clustering\n",
    "    threshold = 1\n",
    "    \n",
    "    #skip plotting, comment next line\n",
    "    fig, axs = plt.subplots(1,5,figsize=(35,6))\n",
    "    \n",
    "    sys.setrecursionlimit(10000) #Required for much larger system as trail20_(5-5) which has 1170 type A molecules\n",
    "    linkage = hierarchy.linkage(distances, method=\"single\")\n",
    "    clusters = hierarchy.fcluster(linkage, threshold, criterion=\"distance\")\n",
    "    dend = hierarchy.dendrogram(linkage, color_threshold=threshold, no_plot=True)#ax=axs[4])\n",
    "    order = dend['leaves']\n",
    "    ORDER = np.append(ORDER,order,axis=0)\n",
    "    distances = distances[order,:]\n",
    "    distances = distances[:,order]\n",
    "    N_connect_total = N_connect_total[order,:]\n",
    "    N_connect_total = N_connect_total[:,order]\n",
    "    Direct_neighbor = Direct_neighbor[order,:]\n",
    "    Direct_neighbor = Direct_neighbor[:,order]\n",
    "    if len(N_CONNECT_TOTAL)==0:\n",
    "        N_CONNECT_TOTAL = N_connect_total\n",
    "    else:\n",
    "        N_CONNECT_TOTAL = np.append(N_CONNECT_TOTAL,N_connect_total,axis=0)\n",
    "\n",
    "    \n",
    "    #Calculate the direct neighbor numbers and the label for each nodes\n",
    "    N_direct_neighbor = [N_neighbor[i] for i in order]\n",
    "    N_direct_neighbor_matrix = np.tile(np.array([N_direct_neighbor]).transpose(),(1,np.int(g.vcount())))\n",
    "    N_direct_neighbor_per = []\n",
    "    N_label = []\n",
    "    for i in range(0,len(order)):\n",
    "        if order[i]<N_A:\n",
    "            N_direct_neighbor_per.append(N_direct_neighbor[i]/2)\n",
    "            N_label.append(1) #'A' is labled as 1\n",
    "        else:\n",
    "            N_direct_neighbor_per.append(N_direct_neighbor[i]/6)\n",
    "            N_label.append(0) #'B' is labled as 0\n",
    "    N_direct_neighbor_per_matrix = np.tile(np.array([N_direct_neighbor_per]).transpose(),(1,np.int(g.vcount())))\n",
    "    N_label_matrix = np.tile(np.array([N_label]).transpose(),(1,np.int(g.vcount())))\n",
    "#   Plot all the analysis results  \n",
    "    #After clustering, convert the largest distances (& not indirectly connected nodes) to nan and plot as white in the colormap\n",
    "    max_index = np.where(distances == np.amax(distances)) \n",
    "    distances[max_index] = np.nan\n",
    "    no_connect_index = np.where(N_connect_total == 0)\n",
    "    N_connect_total[no_connect_index] = np.nan\n",
    "    \n",
    "    #skip plotting for now, uncomment next line\n",
    "    #continue\n",
    "\n",
    "    \n",
    "    current_cmap = plt.cm.get_cmap()\n",
    "    current_cmap.set_bad(color='white')\n",
    "\n",
    "    [x,y] = np.where(Direct_neighbor == 1) #Extract x,y node position for those are direct neighbors\n",
    "    \n",
    "    sc0 = axs[0].imshow(distances)\n",
    "#     sc1 = axs[1].imshow(N_label_matrix)\n",
    "    sc1 = axs[1].scatter(x, y, s=0.1) \n",
    "    sc2 = axs[2].imshow(N_direct_neighbor_per_matrix)\n",
    "    sc3 = axs[3].imshow(N_connect_total)\n",
    "    \n",
    "    cbar0 = fig.colorbar(sc0, ax=axs[0],extend='neither')\n",
    "#     sc0.set_clim(vmin=0,vmax=1.1)\n",
    "\n",
    "    axs[1].set_xlim(0,Direct_neighbor.shape[0])\n",
    "    axs[1].set_ylim(0,Direct_neighbor.shape[0])\n",
    "    axs[1].invert_yaxis()\n",
    "    axs[1].set_aspect('equal')\n",
    "#     cbar1 = fig.colorbar(sc1, ax=axs[1],ticks=[0, 1])\n",
    "# #     cbar1.ax.set_yticklabels(['B', 'A'])\n",
    "#     cbar1.ax.set_yticklabels(['N','Y'])\n",
    "    \n",
    "    cbar2 = fig.colorbar(sc2, ax=axs[2],extend='neither')\n",
    "#     sc2.set_clim(vmin=0,vmax=1)\n",
    "    \n",
    "    cbar3 = fig.colorbar(sc3, ax=axs[3],extend='neither')\n",
    "#     sc3.set_clim(vmin=0,vmax=53)\n",
    "    \n",
    "    axs[2].xaxis.set_visible(False)\n",
    "    cbar0.set_label(r'Topological shortest distance (nm)', fontsize=18)\n",
    "#     cbar1.set_label('Node type', fontsize=14)\n",
    "    axs[1].yaxis.set_label_position('right')\n",
    "    axs[1].set_ylabel('Direct Neighbors', fontsize=18)\n",
    "    cbar2.set_label('precentage of direct neighbors', fontsize=18)\n",
    "    cbar3.set_label('Number of nodes connecting path', fontsize=18)\n",
    "    axs[0].set_title(f't = {df.time[gidx*plot_stride]:.3f} s', fontsize=18)    \n",
    "    axs[3].set_title(f't = {df.time[gidx*plot_stride]:.3f} s', fontsize=18)    \n",
    "    axs[4].set_xlabel(\"Node\",fontsize=19)\n",
    "#     axs[4].set_ylabel(\"Dissimilarity\")\n",
    "#     plt.subplots_adjust(wspace = 0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot individual node clusters in 3d using coordinate values ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "R = [] #first cluster radius\n",
    "MIU2_XY = [] #normalized first central moment on x,y direction, suggesting deviation from circular shape (0 suggests circular)\n",
    "MIU2_XZ = [] #normalized first central moment on x,z direction\n",
    "MIU2_YZ = [] #normalized first central moment on y,z direction\n",
    "Cluster_size = [] #Number of nodes within the first tracked cluster at each time frames\n",
    "#for index, row in tqdm(df.tail(n=5).iterrows(), total=5):\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    g = row['graph']\n",
    "    Coord = g.vs['coordinate']\n",
    "    order = ORDER[index*g.vcount():(index+1)*g.vcount()]\n",
    "    n_connect_total = N_CONNECT_TOTAL[index*g.vcount():(index+1)*g.vcount()]\n",
    "    ccs = g.clusters()\n",
    "    N_cluster = len(ccs) #N_cluster is the number of clusters within each frame\n",
    "    Ni_index = [] #Store the actual node index (not index directly from n_connect_total matrix) before clustering using order array\n",
    "    check_temp_cluster = 0 #Checkpoint for the row index between different squares in graph\n",
    "    if np.any(np.isnan(n_connect_total)): #whether N_CONNECT_TOTAL elements have been replace to NaN for plotting\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if ~np.isnan(e)]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "    else:\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if e!=0]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "\n",
    "    Len_Ni_index = [len(i) for i in Ni_index]\n",
    "    \n",
    "    #Find the cluster index that have the largest number of common node compared to the cluster in the previous frame, track first 3 clusters\n",
    "    if index !=0:\n",
    "        common_node_N1 = []\n",
    "        common_node_N2 = []\n",
    "        common_node_N3 = []\n",
    "        for i_node in Ni_index:\n",
    "            common_node_N1.append(len(set(N1_max_track_index_pre).intersection(set(i_node))))\n",
    "            common_node_N2.append(len(set(N2_max_track_index_pre).intersection(set(i_node))))\n",
    "            common_node_N3.append(len(set(N3_max_track_index_pre).intersection(set(i_node))))\n",
    "        max_index_N1 = common_node_N1.index(max(common_node_N1)) #node cluster index that has largest number of common nodes with cluster in the previous timeframe\n",
    "        max_index_N2 = common_node_N2.index(max(common_node_N2))\n",
    "        max_index_N3 = common_node_N3.index(max(common_node_N3))\n",
    "\n",
    "    #Find first five/six largest cluster within first 10 clusters at each time points\n",
    "    Sort_Len = np.argsort(Len_Ni_index) #Returns the indices that would sort the array in ascending order\n",
    "    N1_max_index = Ni_index[Sort_Len[len(Sort_Len)-1]]\n",
    "    N2_max_index = Ni_index[Sort_Len[len(Sort_Len)-2]]\n",
    "    N3_max_index = Ni_index[Sort_Len[len(Sort_Len)-3]]\n",
    "    N4_max_index = Ni_index[Sort_Len[len(Sort_Len)-4]]\n",
    "    N5_max_index = Ni_index[Sort_Len[len(Sort_Len)-5]]\n",
    "#     N6_max_index = Ni_index[Sort_Len[len(Sort_Len)-6]]\n",
    "\n",
    "    \n",
    "    #Define the connected cluster at this current time frame and is used for comparison at next time frame, track first 3 clusters\n",
    "    if index == 0:\n",
    "        N1_max_track_index_pre = N1_max_index  #Choose the tracking cluster at first frame within range(0,N_cluster), here are the clusters among first five largest clusters in the first frame\n",
    "        N2_max_track_index_pre = N2_max_index\n",
    "        N3_max_track_index_pre = N3_max_index\n",
    "    else: \n",
    "        N1_max_track_index_pre = Ni_index[max_index_N1]\n",
    "        N2_max_track_index_pre = Ni_index[max_index_N2]\n",
    "        N3_max_track_index_pre = Ni_index[max_index_N3]\n",
    "    \n",
    "    Cluster_size.append(len(N1_max_track_index_pre))\n",
    "    \n",
    "    #Plot scatter plot for the first five/six largest clusters at each time points\n",
    "    fig = plt.figure(figsize=(14,5))\n",
    "    ax1 = fig.add_subplot(1,2,1, projection='3d')\n",
    "    N1_max_coord = [Coord[np.int(i)] for i in N1_max_index]\n",
    "    N2_max_coord = [Coord[np.int(i)] for i in N2_max_index]\n",
    "    N3_max_coord = [Coord[np.int(i)] for i in N3_max_index]\n",
    "    N4_max_coord = [Coord[np.int(i)] for i in N4_max_index]\n",
    "    N5_max_coord = [Coord[np.int(i)] for i in N5_max_index]\n",
    "#     N6_max_coord = [Coord[np.int(i)] for i in N6_max_index]  \n",
    "    \n",
    "    #Caculate center of mass based on periodic boundary conditions through mapping x  (or y,z) dimension to a circle\n",
    "    #(https://en.wikipedia.org/wiki/Center_of_mass#Systems_with_periodic_boundary_conditions)    \n",
    "    theta = (np.array(N1_max_coord)+L/2)/L*2*math.pi  #Coordinates are from [0,L], thus plus L/2\n",
    "    sin_theta = np.average(np.sin(theta),axis=0)\n",
    "    cos_theta = np.average(np.cos(theta),axis=0)\n",
    "    com_theta = np.array([math.atan2(-sin_theta[i],-cos_theta[i])+math.pi for i in range(3)])\n",
    "    com = L*com_theta/2/math.pi-L/2 #Center of mass coordinates, substract L/2\n",
    "#     print(np.average((N1_max_coord),axis=0))\n",
    "#     print(com)\n",
    "    \n",
    "    r_vector = np.array(N1_max_coord)-com\n",
    "    r_vector = r_vector - L*np.floor(r_vector/L+0.5)\n",
    "    r = np.average([np.sqrt(np.power(i,2).sum()) for i in r_vector]) #or max(...) or np.median(...) or np.average(...)\n",
    "    miu2_xy = sum([i[0]*i[1] for i in r_vector])/(len(r_vector)^2)\n",
    "    miu2_xz = sum([i[0]*i[2] for i in r_vector])/(len(r_vector)^2)\n",
    "    miu2_yz = sum([i[1]*i[2] for i in r_vector])/(len(r_vector)^2)\n",
    " \n",
    "    R.append(r)\n",
    "    MIU2_XY.append(miu2_xy)\n",
    "    MIU2_XZ.append(miu2_xz)\n",
    "    MIU2_YZ.append(miu2_yz)\n",
    "    \n",
    "    ax1.scatter([i[0] for i in N1_max_coord], [i[1] for i in N1_max_coord], [i[2] for i in N1_max_coord], c='r', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax1.scatter([i[0] for i in N2_max_coord], [i[1] for i in N2_max_coord], [i[2] for i in N2_max_coord], c='b', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax1.scatter([i[0] for i in N3_max_coord], [i[1] for i in N3_max_coord], [i[2] for i in N3_max_coord], c='g', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "    ax1.scatter([i[0] for i in N4_max_coord], [i[1] for i in N4_max_coord], [i[2] for i in N4_max_coord], c='c', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "    ax1.scatter([i[0] for i in N5_max_coord], [i[1] for i in N5_max_coord], [i[2] for i in N5_max_coord], c='m', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "#     ax1.scatter([i[0] for i in N6_max_coord], [i[1] for i in N6_max_coord], [i[2] for i in N6_max_coord], c='y', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "   \n",
    "    ax1.set_title(f't = {df.time[index]:.3f} s')    \n",
    "    ax1.set_zlim(-600,600)\n",
    "    ticks = np.arange(-600, 600, 200)\n",
    "    ax1.set_xticks(ticks)\n",
    "    ax1.set_yticks(ticks)\n",
    "    ax1.set_zticks(ticks)\n",
    "\n",
    "    ax1.set_xlabel(r'Box X (nm)')\n",
    "    ax1.set_ylabel(r'Box Y (nm)')\n",
    "    ax1.set_zlabel(r'Box Z (nm)')\n",
    "    \n",
    "    #Plot scatter plot for the tracked clusters (the first three clusters in the 1st frame) over time \n",
    "    ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "    connected_coord_N1 = [Coord[np.int(i)] for i in N1_max_track_index_pre]\n",
    "    connected_coord_N2 = [Coord[np.int(i)] for i in N2_max_track_index_pre]\n",
    "    connected_coord_N3 = [Coord[np.int(i)] for i in N3_max_track_index_pre]\n",
    "    ax2.scatter([i[0] for i in connected_coord_N1], [i[1] for i in connected_coord_N1], [i[2] for i in connected_coord_N1], c='r', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.scatter([i[0] for i in connected_coord_N2], [i[1] for i in connected_coord_N2], [i[2] for i in connected_coord_N2], c='b', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.scatter([i[0] for i in connected_coord_N3], [i[1] for i in connected_coord_N3], [i[2] for i in connected_coord_N3], c='g', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.set_title(f't = {df.time[index]:.3f} s')\n",
    "    \n",
    "    ax2.set_zlim(-600,600)\n",
    "    ticks = np.arange(-600, 600, 200)\n",
    "    ax2.set_xticks(ticks)\n",
    "    ax2.set_yticks(ticks)\n",
    "    ax2.set_zticks(ticks)\n",
    "\n",
    "    ax2.set_xlabel(r'Box X (nm)')\n",
    "    ax2.set_ylabel(r'Box Y (nm)')\n",
    "    ax2.set_zlabel(r'Box Z (nm)')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Plot 0th and 1st moment with time for the largest cluster, indicating its actual radius and circularity in x/y/z plane\n",
    "fig, axs = plt.subplots(1,2, figsize=(15,6))\n",
    "axs[0].plot(df.time,R)\n",
    "axs[0].set_ylabel(r'Radius of first cluster (nm)', fontsize=18)\n",
    "axs[0].set_xlabel('Time (s)', fontsize=18)\n",
    "# axs[0].set_ylim(0,0.18)\n",
    "\n",
    "axs[1].plot(df.time,MIU2_XY,color='blue', label='X,Y direction')\n",
    "axs[1].plot(df.time,MIU2_XZ,color='red', label='X,Z direction')\n",
    "axs[1].plot(df.time,MIU2_YZ,color='green', label='Y,Z direction')\n",
    "axs[1].set_ylabel('Normalized first central moment', fontsize=18)\n",
    "axs[1].set_xlabel('Time (s)', fontsize=18)\n",
    "axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "# axs[1].set_ylim(-0.012,0.012)\n",
    "\n",
    "#Plot the number of nodes within the first tracked cluster over timed\n",
    "dt = np.diff(df.time)\n",
    "dCs = np.diff(Cluster_size)\n",
    "fig,axs = plt.subplots(1,2, figsize=(15,6))\n",
    "axs[0].plot(df.time,Cluster_size)\n",
    "axs[0].set_ylabel(r'Number of nodes within first tracked cluster', fontsize=18)\n",
    "axs[0].set_xlabel('Time (s)', fontsize=18)\n",
    "# ax[0].set_ylim(0,280)\n",
    "\n",
    "axs[1].plot(df.time[1:],[x/y for x, y in zip(dCs, dt)])\n",
    "axs[1].set_ylabel(r'First derivation of node number', fontsize=18)\n",
    "axs[1].set_xlabel('Time (s)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added Cell 1: Plot individual node clusters in 3d using coordinate values by plotting 5 or 6 largest clusters at each time points or tracked clusters from first 3 largest clusters in the last frame.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [] #first cluster radius\n",
    "MIU2_XY = [] #normalized first central moment on x,y direction, suggesting deviation from circular shape (0 suggests circular)\n",
    "MIU2_XZ = [] #normalized first central moment on x,z direction\n",
    "MIU2_YZ = [] #normalized first central moment on y,z direction\n",
    "Cluster_size = [] #Number of nodes within the first tracked cluster at each time frames\n",
    "\n",
    "df_copy = df.copy()  #Copy dataframe 'df' to a new dataframe 'df_copy' to avoid changing original dataframe\n",
    "reversed_df = df_copy.loc[::-1] #Reverse copied dataframe in terms of index and saved to 'reversed_df' dataframe\n",
    "# for index, row in tqdm(reversed_df.head(n=5).iterrows(), total=5):\n",
    "for index, row in tqdm(reversed_df.iterrows(), total=reversed_df.shape[0]):\n",
    "    if not (index%plot_stride == 0): continue\n",
    "    if index==len(reversed_df)-1: continue\n",
    "    \n",
    "    g = row['graph']\n",
    "    Coord = g.vs['coordinate']\n",
    "    order = ORDER[index*g.vcount():(index+1)*g.vcount()]\n",
    "    n_connect_total = N_CONNECT_TOTAL[index*g.vcount():(index+1)*g.vcount()]\n",
    "    \n",
    "    ccs = g.clusters()\n",
    "    N_cluster = len(ccs) #N_cluster is the number of clusters within each frame\n",
    "    Ni_index = [] #Store the actual node index (not index directly from n_connect_total matrix) before clustering using order array\n",
    "    check_temp_cluster = 0 #Checkpoint for the row index between different squares in graph\n",
    "    if np.any(np.isnan(n_connect_total)): #whether N_CONNECT_TOTAL elements have been replace to NaN for plotting\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if ~np.isnan(e)]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "    else:\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if e!=0]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "\n",
    "    Len_Ni_index = [len(i) for i in Ni_index]\n",
    "    \n",
    "    #Find the cluster index that have the largest number of common node compared to the cluster in the previous frame, track first 3 clusters\n",
    "    if index != reversed_df.shape[0]-1:\n",
    "        common_node_N1 = []\n",
    "        common_node_N2 = []\n",
    "        common_node_N3 = []\n",
    "        for i_node in Ni_index:\n",
    "            common_node_N1.append(len(set(N1_max_track_index_pre).intersection(set(i_node))))\n",
    "            common_node_N2.append(len(set(N2_max_track_index_pre).intersection(set(i_node))))\n",
    "            common_node_N3.append(len(set(N3_max_track_index_pre).intersection(set(i_node))))\n",
    "        max_index_N1 = common_node_N1.index(max(common_node_N1)) #node cluster index that has largest number of common nodes with cluster in the previous timeframe\n",
    "        max_index_N2 = common_node_N2.index(max(common_node_N2))\n",
    "        max_index_N3 = common_node_N3.index(max(common_node_N3))\n",
    "\n",
    "    #Find first five/six largest cluster at each time points\n",
    "    Sort_Len = np.argsort(Len_Ni_index) #Returns the indices that would sort the array in ascending order\n",
    "    N1_max_index = Ni_index[Sort_Len[len(Sort_Len)-1]]\n",
    "    N2_max_index = Ni_index[Sort_Len[len(Sort_Len)-2]]\n",
    "    N3_max_index = Ni_index[Sort_Len[len(Sort_Len)-3]]\n",
    "    N4_max_index = Ni_index[Sort_Len[len(Sort_Len)-4]]\n",
    "    N5_max_index = Ni_index[Sort_Len[len(Sort_Len)-5]]\n",
    "#     N6_max_index = Ni_index[Sort_Len[len(Sort_Len)-6]]\n",
    "\n",
    "    \n",
    "    #Define the connected cluster at this current time frame and is used for comparison at next time frame, track first 3 clusters\n",
    "    if index == reversed_df.shape[0]-1:\n",
    "        N1_max_track_index_pre = N1_max_index  #Choose the tracking cluster at first frame within range(0,N_cluster), here are the clusters among first five largest clusters in the first frame\n",
    "        N2_max_track_index_pre = N2_max_index\n",
    "        N3_max_track_index_pre = N3_max_index\n",
    "    else: \n",
    "        N1_max_track_index_pre = Ni_index[max_index_N1]\n",
    "        N2_max_track_index_pre = Ni_index[max_index_N2]\n",
    "        N3_max_track_index_pre = Ni_index[max_index_N3]\n",
    "    \n",
    "    Cluster_size.append(len(N1_max_track_index_pre))\n",
    "    \n",
    "    #Plot scatter plot for the first five/six largest clusters at each time points\n",
    "    fig = plt.figure(figsize=(14,5))\n",
    "    ax1 = fig.add_subplot(1,2,1, projection='3d')\n",
    "    N1_max_coord = [Coord[np.int(i)] for i in N1_max_index]\n",
    "    N2_max_coord = [Coord[np.int(i)] for i in N2_max_index]\n",
    "    N3_max_coord = [Coord[np.int(i)] for i in N3_max_index]\n",
    "    N4_max_coord = [Coord[np.int(i)] for i in N4_max_index]\n",
    "    N5_max_coord = [Coord[np.int(i)] for i in N5_max_index]\n",
    "#     N6_max_coord = [Coord[np.int(i)] for i in N6_max_index]  \n",
    "    \n",
    "    #Caculate center of mass based on periodic boundary conditions through mapping x  (or y,z) dimension to a circle\n",
    "    #(https://en.wikipedia.org/wiki/Center_of_mass#Systems_with_periodic_boundary_conditions)    \n",
    "    theta = (np.array(N1_max_coord)+L/2)/L*2*math.pi  #Coordinates are from [0,L], thus plus L/2\n",
    "    sin_theta = np.average(np.sin(theta),axis=0)\n",
    "    cos_theta = np.average(np.cos(theta),axis=0)\n",
    "    com_theta = np.array([math.atan2(-sin_theta[i],-cos_theta[i])+math.pi for i in range(3)])\n",
    "    com = L*com_theta/2/math.pi-L/2 #Center of mass coordinates, substract L/2\n",
    "#     print(np.average((N1_max_coord),axis=0))\n",
    "#     print(com)\n",
    "    \n",
    "    r_vector = np.array(N1_max_coord)-com\n",
    "    r_vector = r_vector - L*np.floor(r_vector/L+0.5)\n",
    "    r = np.average([np.sqrt(np.power(i,2).sum()) for i in r_vector]) #or max(...) or np.median(...) or np.average(...)\n",
    "    miu2_xy = sum([i[0]*i[1] for i in r_vector])/(len(r_vector)^2)\n",
    "    miu2_xz = sum([i[0]*i[2] for i in r_vector])/(len(r_vector)^2)\n",
    "    miu2_yz = sum([i[1]*i[2] for i in r_vector])/(len(r_vector)^2)\n",
    " \n",
    "    R.append(r)\n",
    "    MIU2_XY.append(miu2_xy)\n",
    "    MIU2_XZ.append(miu2_xz)\n",
    "    MIU2_YZ.append(miu2_yz)\n",
    "    \n",
    "    ax1.scatter([i[0] for i in N1_max_coord], [i[1] for i in N1_max_coord], [i[2] for i in N1_max_coord], c='r', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax1.scatter([i[0] for i in N2_max_coord], [i[1] for i in N2_max_coord], [i[2] for i in N2_max_coord], c='b', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax1.scatter([i[0] for i in N3_max_coord], [i[1] for i in N3_max_coord], [i[2] for i in N3_max_coord], c='g', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "    ax1.scatter([i[0] for i in N4_max_coord], [i[1] for i in N4_max_coord], [i[2] for i in N4_max_coord], c='c', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "    ax1.scatter([i[0] for i in N5_max_coord], [i[1] for i in N5_max_coord], [i[2] for i in N5_max_coord], c='m', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "#     ax1.scatter([i[0] for i in N6_max_coord], [i[1] for i in N6_max_coord], [i[2] for i in N6_max_coord], c='y', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "   \n",
    "    ax1.set_title(f't = {reversed_df.time[index]:.3f} s')    \n",
    "    ax1.set_zlim(-600,600)\n",
    "    ticks = np.arange(-600, 600, 200)\n",
    "    ax1.set_xticks(ticks)\n",
    "    ax1.set_yticks(ticks)\n",
    "    ax1.set_zticks(ticks)\n",
    "\n",
    "    ax1.set_xlabel(r'Box X (nm)')\n",
    "    ax1.set_ylabel(r'Box Y (nm)')\n",
    "    ax1.set_zlabel(r'Box Z (nm)')\n",
    "    \n",
    "    #Plot scatter plot for the tracked clusters (the first three clusters in the last frame) over time \n",
    "    ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "    connected_coord_N1 = [Coord[np.int(i)] for i in N1_max_track_index_pre]\n",
    "    connected_coord_N2 = [Coord[np.int(i)] for i in N2_max_track_index_pre]\n",
    "    connected_coord_N3 = [Coord[np.int(i)] for i in N3_max_track_index_pre]\n",
    "    ax2.scatter([i[0] for i in connected_coord_N1], [i[1] for i in connected_coord_N1], [i[2] for i in connected_coord_N1], c='r', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.scatter([i[0] for i in connected_coord_N2], [i[1] for i in connected_coord_N2], [i[2] for i in connected_coord_N2], c='b', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.scatter([i[0] for i in connected_coord_N3], [i[1] for i in connected_coord_N3], [i[2] for i in connected_coord_N3], c='g', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.set_title(f't = {reversed_df.time[index]:.3f} s')\n",
    "    \n",
    "    ax2.set_zlim(-600,600)\n",
    "    ticks = np.arange(-600, 600, 200)\n",
    "    ax2.set_xticks(ticks)\n",
    "    ax2.set_yticks(ticks)\n",
    "    ax2.set_zticks(ticks)\n",
    "\n",
    "    ax2.set_xlabel(r'Box X (nm)')\n",
    "    ax2.set_ylabel(r'Box Y (nm)')\n",
    "    ax2.set_zlabel(r'Box Z (nm)')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Plot 0th and 1st moment with time for the largest cluster, indicating its actual radius and circularity in x/y/z plane\n",
    "fig, axs = plt.subplots(1,2, figsize=(15,6))\n",
    "axs[0].plot(reversed_df.time,R)\n",
    "axs[0].set_ylabel(r'Radius of first cluster (nm)', fontsize=18)\n",
    "axs[0].set_xlabel('Time (s)', fontsize=18)\n",
    "# axs[0].set_ylim(0,0.18)\n",
    "\n",
    "axs[1].plot(reversed_df.time,MIU2_XY,color='blue', label='X,Y direction')\n",
    "axs[1].plot(reversed_df.time,MIU2_XZ,color='red', label='X,Z direction')\n",
    "axs[1].plot(reversed_df.time,MIU2_YZ,color='green', label='Y,Z direction')\n",
    "axs[1].set_ylabel('Normalized first central moment', fontsize=18)\n",
    "axs[1].set_xlabel('Time (s)', fontsize=18)\n",
    "axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "# axs[1].set_ylim(-0.012,0.012)\n",
    "\n",
    "##Plot the number of nodes within the first tracked cluster over timed\n",
    "#dt = np.diff(reversed_df.time)\n",
    "#dCs = np.diff(Cluster_size)\n",
    "#fig,axs = plt.subplots(1,2, figsize=(15,6))\n",
    "#axs[0].plot(reversed_df.time,Cluster_size)\n",
    "#axs[0].set_ylabel(r'Number of nodes within first tracked cluster', fontsize=18)\n",
    "#axs[0].set_xlabel('Time (s)', fontsize=18)\n",
    "## ax[0].set_ylim(0,280)\n",
    "\n",
    "#axs[1].plot(df.time[1:],[x/y for x, y in zip(dCs, dt)])\n",
    "#axs[1].set_ylabel(r'First derivation of node number', fontsize=18)\n",
    "#axs[1].set_xlabel('Time (s)', fontsize=18)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the neighbor properties of all nodes as well as the nodes within the largest cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexA = np.array(range(N_A)) #Node index number for typeA molecules\n",
    "indexB = np.array(range(N_A,N_A+N_B)) #Node index number for typeB molecules\n",
    "\n",
    "N_neighbor_node = [] #For each node, number of neighbors at all time points\n",
    "N_neighbor_node_percen = [] #For each node, percentage of neighbors number relative to all available binding sites\n",
    "# node_label = [None]*len(N_neighbor_time[0]) #initialize the node_label with total number of A/B nodes\n",
    "for node_index in range(0,len(N_neighbor_time[0])):\n",
    "    N_neighbor_indi_node = [N_neighbor_time[time_index][node_index] for time_index in range(0,len(N_neighbor_time))]\n",
    "    #number of neighbors for each individual nodes at all time points\n",
    "    N_neighbor_node.append(N_neighbor_indi_node)\n",
    "    if node_index in indexA:\n",
    "#         node_label[node_index] = 'A'\n",
    "        N_neighbor_node_percen.append([x/2 for x in N_neighbor_indi_node])\n",
    "    else:\n",
    "#         node_label[node_index] = 'B'\n",
    "        N_neighbor_node_percen.append([x/6 for x in N_neighbor_indi_node])\n",
    "\n",
    "    \n",
    "N_neighbor_node_giant = [] #Initialize average neighbor numbers for nodes within largest cluster\n",
    "N_neighbor_node_giant_std = [] #std for neighbor numbers of nodes within largest cluster\n",
    "for time_index in range(0,len(giant_node)):\n",
    "    N_giant_node = giant_node[time_index]\n",
    "    N_neighbor_tempA = []\n",
    "    N_neighbor_tempB = []\n",
    "    for i in N_giant_node: \n",
    "        if i in indexA:\n",
    "            N_neighbor_tempA.append(N_neighbor_node[i][time_index])\n",
    "        else:\n",
    "            N_neighbor_tempB.append(N_neighbor_node[i][time_index])\n",
    "    N_neighbor_node_giant.append([np.average(N_neighbor_tempA),np.average(N_neighbor_tempB)])\n",
    "    N_neighbor_node_giant_std.append([np.std(N_neighbor_tempA),np.std(N_neighbor_tempB)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot number of neighbors for all nodes or nodes within largest cluster over time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of neighbors for all nodes over time\n",
    "num_frames = len(df)\n",
    "maxlags = min(num_frames-1,100)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "# ax.set_ylim(0,6.5)\n",
    "ax.plot(df.time,np.average([N_neighbor_node[i] for i in indexA],axis=0), color='blue',label='dimer neighbors')\n",
    "ax.plot(df.time,np.average([N_neighbor_node[i] for i in indexB],axis=0), color='red',label='hexamer neighbors')\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = np.average([N_neighbor_node[i] for i in indexA],axis=0)-np.std([N_neighbor_node[i] for i in indexA],axis=0),\n",
    "    y2 = np.average([N_neighbor_node[i] for i in indexA],axis=0)+np.std([N_neighbor_node[i] for i in indexA],axis=0), alpha=0.3, facecolor='gray')\n",
    "\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = np.average([N_neighbor_node[i] for i in indexB],axis=0)-np.std([N_neighbor_node[i] for i in indexB],axis=0),\n",
    "    y2 = np.average([N_neighbor_node[i] for i in indexB],axis=0)+np.std([N_neighbor_node[i] for i in indexB],axis=0), alpha=0.3, color='gray')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "ax.set_ylabel('Number of neighbors', fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "plt.title('Average of all nodes')\n",
    "plt.show()\n",
    "\n",
    "# Plot number of neighbors for nodes within largest cluster over time\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "# ax.set_ylim(0,6.5)\n",
    "ax.plot(df.time,[N_neighbor_node_giant[i][0] for i in range(0,len(N_neighbor_node_giant))], color='blue',label='dimer neighbors')\n",
    "ax.plot(df.time,[N_neighbor_node_giant[i][1] for i in range(0,len(N_neighbor_node_giant))], color='red',label='hexamer neighbors')\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = [N_neighbor_node_giant[i][0]-N_neighbor_node_giant_std[i][0] for i in range(0,len(N_neighbor_node_giant))],\n",
    "    y2 = [N_neighbor_node_giant[i][0]+N_neighbor_node_giant_std[i][0] for i in range(0,len(N_neighbor_node_giant))], alpha=0.3, facecolor='gray')\n",
    "\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = [N_neighbor_node_giant[i][1]-N_neighbor_node_giant_std[i][1] for i in range(0,len(N_neighbor_node_giant))],\n",
    "    y2 = [N_neighbor_node_giant[i][1]+N_neighbor_node_giant_std[i][1] for i in range(0,len(N_neighbor_node_giant))], alpha=0.3, facecolor='gray')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "ax.set_ylabel('Number of neighbors', fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "plt.title('Average of the nodes within largest cluster')\n",
    "plt.show()\n",
    "\n",
    "# Plot autocorrelation of number of neighbors for all nodes over time\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,4), sharey=True)\n",
    "# ax.set_ylim(0,6.5)\n",
    "axes[0].acorr(np.average([N_neighbor_node[i] for i in indexA],axis=0), maxlags = maxlags, color='blue',label='dimer neighbors')\n",
    "axes[1].acorr(np.average([N_neighbor_node[i] for i in indexB],axis=0), maxlags = maxlags, color='red',label='hexamer neighbors') \n",
    "# axes.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "axes[0].set_ylabel(f'Autocorrelation of \\n node neighbor numbers', fontsize=18)\n",
    "axes[0].set_xlabel('Time lag', fontsize=18)\n",
    "axes[1].set_xlabel('Time lag', fontsize=18)\n",
    "axes[0].set_title('Average of all dimer neighbors')\n",
    "axes[1].set_title('Average of all hexamer neighbors')\n",
    "plt.show()\n",
    "\n",
    "# Plot autocorrelation of number of neighbors of nodes within largest cluster in the last frame over time\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,4), sharey=True)\n",
    "Final_giant_node = giant_node[-1]\n",
    "Final_giant_neighbor_A = [] #For each typeA node (dimer) within last giant cluster, number of neighbors at all time points\n",
    "Final_giant_neighbor_B = [] #For each typeB node (hexamer) within last giant cluster, number of neighbors at all time points\n",
    "for node_index in Final_giant_node:\n",
    "    if node_index in indexA:\n",
    "        Final_giant_neighbor_A.append(N_neighbor_node[node_index])\n",
    "    else:\n",
    "        Final_giant_neighbor_B.append(N_neighbor_node[node_index])\n",
    "axes[0].acorr(np.average(Final_giant_neighbor_A,axis=0), maxlags = maxlags, color='blue',label='dimer neighbors')\n",
    "axes[1].acorr(np.average(Final_giant_neighbor_B,axis=0), maxlags = maxlags, color='red',label='hexamer neighbors') \n",
    "# axes.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "axes[0].set_ylabel(f'Autocorrelation of \\n node neighbor numbers \\n within lart largest giant cluster', fontsize=18)\n",
    "axes[0].set_xlabel('Time lag', fontsize=18)\n",
    "axes[1].set_xlabel('Time lag', fontsize=18)\n",
    "axes[0].set_title(f'Average of dimer neighbors \\n within last largest giant cluster')\n",
    "axes[1].set_title(f'Average of hexamer neighbors \\n within last largest giant cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot colormap distribution of percentage neighbors and diffusivity for nodes within largest cluster\n",
    "cmap = plt.cm.get_cmap('jet')\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "# for index, row in tqdm(df.head(n=5).iterrows(), total=5):\n",
    "    if not (index%plot_stride == 0 or index==len(df.graph)-1): continue\n",
    "\n",
    "    g = row['graph']\n",
    "    Coord = g.vs['coordinate']\n",
    "    N_giant_node = giant_node[index]\n",
    "    N_giant_coord = [Coord[i] for i in N_giant_node]\n",
    "    N_giant_node_percen = [N_neighbor_node_percen[i][index] for i in N_giant_node]\n",
    "    N_giant_diffusion = [g.vs['diffusivity'][i] for i in N_giant_node]\n",
    "\n",
    "    N_giant_node_A = []\n",
    "    N_giant_node_B = []\n",
    "    for i in range(0, len(N_giant_node)):\n",
    "        if N_giant_node[i] in indexA:\n",
    "            N_giant_node_A.append(N_giant_node[i])\n",
    "        else:\n",
    "            N_giant_node_B.append(N_giant_node[i])\n",
    "    N_giant_coord_A = [Coord[i] for i in N_giant_node_A]\n",
    "    N_giant_coord_B = [Coord[i] for i in N_giant_node_B]\n",
    "    N_giant_node_percen_A = [N_neighbor_node_percen[i][index] for i in N_giant_node_A]\n",
    "    N_giant_node_percen_B = [N_neighbor_node_percen[i][index] for i in N_giant_node_B]\n",
    "    N_giant_diffusion_A = [g.vs['diffusivity'][i] for i in N_giant_node_A]\n",
    "    N_giant_diffusion_B = [g.vs['diffusivity'][i] for i in N_giant_node_B]\n",
    "    \n",
    "    #First subplot - 3d node images\n",
    "    fig = plt.figure(figsize=(30,4))\n",
    "    gs = gridspec.GridSpec(1, 5, width_ratios=[1.5, 1.2, 1.2, 0.8, 0.8]) \n",
    "    ax3D = fig.add_subplot(gs[0], projection='3d')\n",
    "    sc3D = ax3D.scatter([i[0] for i in N_giant_coord], [i[1] for i in N_giant_coord], [i[2] for i in N_giant_coord], c=[i for i in N_giant_node_percen], cmap=cmap, marker='o', alpha=0.7, s=8, linewidths=0, depthshade=0)    \n",
    "    ax3D.set_title(f't = {df.time[index]:.3f} s')    \n",
    "    cbar = plt.colorbar(sc3D,extend='neither')\n",
    "    sc3D.set_clim(vmin=0,vmax=1)\n",
    "    cbar.set_label('percentage of direct neighbors', fontsize=14)\n",
    "    \n",
    "    def forceUpdate(event): #Solve the problem of point color changing in 3d scatter plot compared to 2d scatter plot\n",
    "        global sc3D\n",
    "        sc3D.changed()\n",
    "    fig.canvas.mpl_connect('draw_event', forceUpdate)\n",
    "\n",
    "    ax3D.set_xlim(-500,500)\n",
    "    ax3D.set_ylim(-500,500)\n",
    "    ax3D.set_zlim(-500,500)\n",
    "    ticks = np.arange(-500, 500, 200)\n",
    "    ax3D.set_xticks(ticks)\n",
    "    ax3D.set_yticks(ticks)\n",
    "    ax3D.set_zticks(ticks)\n",
    "\n",
    "    ax3D.set_xlabel(r'Box X (nm)')\n",
    "    ax3D.set_ylabel(r'Box Y (nm)')\n",
    "    ax3D.set_zlabel(r'Box Z (nm)')\n",
    "    \n",
    "    #Second subplot - projection on x/y aixs node images with noder neighbor percentage as colorcode\n",
    "    ax = fig.add_subplot(gs[1])\n",
    "    sc = ax.scatter([i[0] for i in N_giant_coord], [i[1] for i in N_giant_coord], c=[i for i in N_giant_node_percen], cmap=cmap, marker='o', s=5, linewidths=0)    \n",
    "    ax.set_title(f't = {df.time[index]:.3f} s')    \n",
    "    cbar = plt.colorbar(sc,extend='neither')\n",
    "    sc.set_clim(vmin=0,vmax=1)\n",
    "    cbar.set_label('percentage of direct neighbors', fontsize=14)\n",
    "\n",
    "    ax.set_xlim(-500,500)\n",
    "    ax.set_ylim(-500,500)\n",
    "    ticks = np.arange(-500, 500, 200)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "    ax.set_xlabel(r'Box X (nm)')\n",
    "    ax.set_ylabel(r'Box Y (nm)')\n",
    "    \n",
    "    #Third subplot - projection on x/y aixs node images with diffusivity as colorcode\n",
    "    ax = fig.add_subplot(gs[2])\n",
    "    sc = ax.scatter([i[0] for i in N_giant_coord], [i[1] for i in N_giant_coord], c=[g.vs['diffusivity'][i] for i in N_giant_node], cmap=cmap, marker='o', s=5, linewidths=0)    \n",
    "    ax.set_title(f't = {df.time[index]:.3f} s')    \n",
    "    cbar = plt.colorbar(sc,extend='neither')\n",
    "#     sc.set_clim(vmin=0,vmax=0.25)\n",
    "    cbar.set_label(r'Avg. node diffusivity ($\\mu m^2/s$)', fontsize=14)\n",
    "\n",
    "    ax.set_xlim(-500,500)\n",
    "    ax.set_ylim(-500,500)\n",
    "    ticks = np.arange(-500, 500, 200)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "    ax.set_xlabel(r'Box X (nm)')\n",
    "    ax.set_ylabel(r'Box Y (nm)')\n",
    "    \n",
    "    #Forth subplot - A/B node separation projection on x/y aixs\n",
    "    ax = fig.add_subplot(gs[3])\n",
    "    sc = ax.scatter([i[0] for i in N_giant_coord_A], [i[1] for i in N_giant_coord_A], c='m', cmap=cmap, marker='o', label='A: Dimer', s=5,alpha=0.5)    \n",
    "    sc = ax.scatter([i[0] for i in N_giant_coord_B], [i[1] for i in N_giant_coord_B], c='y', cmap=cmap, marker='o', label='B: Hexamer', s=5, alpha=0.5)    \n",
    "    ax.set_title(f't = {df.time[index]:.3f} s')    \n",
    "\n",
    "#     ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "\n",
    "    ax.set_xlim(-500,500)\n",
    "    ax.set_ylim(-500,500)\n",
    "    ticks = np.arange(-500, 500, 200)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "    ax.set_xlabel(r'Box X (nm)')\n",
    "    ax.set_ylabel(r'Box Y (nm)')\n",
    "\n",
    "    #Fifth subplot - Plots of number of neighbors for each node vs. node diffusivity\n",
    "    ax = fig.add_subplot(gs[4])\n",
    "    sc = ax.plot(N_giant_node_percen_A, N_giant_diffusion_A, 'mo', label='A: Dimer',alpha=0.5)    \n",
    "    sc = ax.plot(N_giant_node_percen_B, N_giant_diffusion_B, 'yo', label='B: Hexamer', alpha=0.5)    \n",
    "    ax.set_title(f't = {df.time[index]:.3f} s')    \n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "    ax.set_ylim(-0.005,0.25)\n",
    "\n",
    "    ax.set_xlabel(r'Percentage of node neighbors')\n",
    "    ax.set_ylabel(r'Avg. node diffusivity ($\\mu m^2/s$)')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating topological properties of largest cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['others_size'] = None\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        \n",
    "    g = row['graph']\n",
    "    \n",
    "    # Calculate connected compoents (CCs)\n",
    "    ccs = g.clusters()\n",
    "    \n",
    "    # Get giant component\n",
    "    giant = ccs.giant()\n",
    "\n",
    "#     # Size of giant component\n",
    "    df.loc[index,'giant_size'] = giant.vcount()\n",
    "\n",
    "    # Size of giant component in terms of #connections\n",
    "#     df.loc[index,'giant_size'] = giant.ecount()\n",
    "    \n",
    "    # Diameter of giant connected component\n",
    "    # Diameters is defined as the longest shortest path\n",
    "    # between two pairs of nodes\n",
    "#     df.loc[index,'giant_diameter'] = giant.diameter()\n",
    "    df.loc[index,'giant_diameter'] = giant.diameter(weights=giant.es['length'])\n",
    "    \n",
    "    # Std and mean of sizes of remaining components\n",
    "    sizes = ccs.sizes()\n",
    "#     sizes = [g.ecount() for g in ccs.subgraphs()]\n",
    "\n",
    "    # Exclude giant cc\n",
    "    sizes.pop(sizes.index(giant.vcount()))\n",
    "#     sizes.pop(sizes.index(giant.ecount()))\n",
    "    df.loc[index,'mean_others_size'] = np.mean(sizes) if sizes else None\n",
    "    df.loc[index,'std_others_size'] = np.std(sizes) if sizes else None\n",
    "    \n",
    "    # Frequency of clusters with size 0, 1, 2..\n",
    "    hist = np.bincount(sizes)\n",
    "    df.at[index,'others_size'] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ymax = 1.1*df.giant_size.max()\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(6,4))\n",
    "ax1.plot(df.time,df.giant_size, color='blue', label='Number of nodes within largest cluster')\n",
    "ax1.set_ylabel(\"Number of nodes \\n within largest cluster\", fontsize=18, color='blue')\n",
    "ax1.set_xlabel('Time (s)', fontsize=18)\n",
    "# ax1.set_ylim(0,480)\n",
    "plt.show()\n",
    "\n",
    "fig, ax2 = plt.subplots(1,1, figsize=(6,4))\n",
    "ax2.set_ylabel('Topological diameter \\n of largest cluster (nm)', fontsize=18, color='red')\n",
    "ax2.plot(df.time,df.giant_diameter, color='red', label='Topological diameter of largest cluster')\n",
    "# ax2.set_ylim(0,1.2)\n",
    "# ax1.legend(loc='center left', bbox_to_anchor=(1.15, 0.9))\n",
    "# ax2.legend(loc='center left', bbox_to_anchor=(1.15, 0.8))\n",
    "plt.show\n",
    "# fig.savefig('./Ensemble-node-links/box_Viscosity_trail19_(1)_nodelinks/giant cluster trail19 (1).pdf')\n",
    "\n",
    "#Plot first derivative of nodes number within largest cluster vs. time\n",
    "dt = np.diff(df.time)\n",
    "dGDs = np.diff(df.giant_size)\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.plot(df.time[1:],[x/y for x, y in zip(dGDs, dt)])\n",
    "ax.set_ylabel(\"First derivative of node number \\n within largest cluster\", fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe with time and file paths for links and coordinates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = df.time.max()\n",
    "ymax = 1.1*df.giant_size.max()\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.set_ylabel('Size of remaining CCs', fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "ax.set_xlim(0,xmax)\n",
    "# ax.set_ylim(0,30)\n",
    "df_sub = df.dropna()\n",
    "ax.fill_between(\n",
    "    df_sub.time,\n",
    "    y1 = df_sub.mean_others_size-df_sub.std_others_size,\n",
    "    y2 = df_sub.mean_others_size+df_sub.std_others_size, alpha=0.3, color='gray')\n",
    "ax.plot(df_sub.time,df_sub.mean_others_size, '-o', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution of clusters sizes as a heatmap\n",
    "# This does not include the giant cluster\n",
    "\n",
    "# Calculate the maximum number of co-occurent clusters\n",
    "nmax = []\n",
    "for index in df.index:\n",
    "    hist = df.others_size[index]\n",
    "    if hist.size:\n",
    "        nmax.append(len(hist))\n",
    "# Empty heatmap\n",
    "data = np.zeros((np.max(nmax),df.shape[0]), dtype=np.uint64)\n",
    "# Fill heatmap in\n",
    "for i, index in enumerate(df.index):\n",
    "    hist = df.others_size[index]\n",
    "    data[:len(hist),i] = hist\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,4))\n",
    "sc0 = ax.imshow(np.log(1+data[:40,:]),cmap='jet')\n",
    "# sc0 = ax.imshow(data,cmap='jet')\n",
    "# ax.set_xlabel(f'Time (s x {df.shape[0]})', fontsize=18)\n",
    "# ax.set_xlabel('Time (s x %1.2f)' %np.diff(df_sub.time)[0], fontsize=18)\n",
    "ax.set_xlabel(f'Time (x {np.diff(df_sub.time)[0]:.3f} s)', fontsize=18)\n",
    "ax.set_ylabel('Cluster size', fontsize=18)\n",
    "clbar = fig.colorbar(sc0,extend='neither')\n",
    "# sc0.set_clim(vmin=0,vmax=6.8)\n",
    "clbar.set_label('ln(1 + #Clusters)',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraction of neighboors that remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxi, idxj in tqdm(zip(df.index[:-1],df.index[1:]), total=df.shape[0]-1):\n",
    "    \n",
    "    # Load graph at time   t: gi\n",
    "    # Load graph at time t+1: gj\n",
    "    gi = df.graph[idxi]\n",
    "    gj = df.graph[idxj]\n",
    "    \n",
    "    # Get list of neighbors\n",
    "    neighi = gi.neighborhood()\n",
    "    neighj = gj.neighborhood()\n",
    "        \n",
    "    fraction = []\n",
    "    \n",
    "    # Check the fraction of nodes with unchanged neighborhood\n",
    "    for nik, njk in zip(neighi,neighj):\n",
    "        \n",
    "        # Intersection between the two sets of neighs\n",
    "        common = set(njk).intersection(set(nik))\n",
    "        \n",
    "        fraction.append(len(common) / np.max([len(nik),len(njk)]))\n",
    "        \n",
    "    df.loc[idxj,'frac_sim_neighs_avg'] = np.mean(fraction)\n",
    "    df.loc[idxj,'frac_sim_neighs_std'] =  np.std(fraction)\n",
    "\n",
    "# Plot results\n",
    "xmax = df.time.max()\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.set_ylabel('Fraction of unchanged neighbors', fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "ax.set_xlim(0,xmax)\n",
    "# ax.set_ylim(0.6,1.1)\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = df.frac_sim_neighs_avg-df.frac_sim_neighs_std,\n",
    "    y2 = df.frac_sim_neighs_avg+df.frac_sim_neighs_std, alpha=0.3, color='gray')\n",
    "ax.plot(df.time,df.frac_sim_neighs_avg, '-o', color='black')\n",
    "\n",
    "# Plot zoom-in results from 0-0.25s\n",
    "fig, ax_zoom = plt.subplots(1,1, figsize=(1,4))\n",
    "# ax_zoom.set_ylabel('Fraction of unchanged neighbors', fontsize=18)\n",
    "ax_zoom.set_xlabel('Time (s)', fontsize=18)\n",
    "# ax_zoom.set_xlim(0,0.25)\n",
    "# ax_zoom.set_ylim(0.6,1.1)\n",
    "ax_zoom.fill_between(\n",
    "    df.time[0:25],\n",
    "    y1 = df.frac_sim_neighs_avg[0:25]-df.frac_sim_neighs_std[0:25],\n",
    "    y2 = df.frac_sim_neighs_avg[0:25]+df.frac_sim_neighs_std[0:25], alpha=0.3, color='gray')\n",
    "ax_zoom.plot(df.time[0:25],df.frac_sim_neighs_avg[0:25], '-o', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time aggregated graph and hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of graphs to be aggregated together\n",
    "n_agg_graphs = 5 # how many time frames (in the selected nodelink files) are used for each aggregation, not second in unit\n",
    "\n",
    "times = df['time'].values\n",
    "tstep = np.median(np.diff(df.time.values))\n",
    "nbins = np.int(np.round(len(times)/n_agg_graphs))+1\n",
    "\n",
    "# nbins = np.int(np.round((times.max()-times.min())/(n_agg_graphs*tstep)))+1\n",
    "# df['agg_time'] = np.digitize(\n",
    "#     times,\n",
    "#     np.linspace(times.min(),times.max(),nbins)\n",
    "# )\n",
    "\n",
    "df['agg_index'] = np.digitize(\n",
    "    range(0,len(times)),\n",
    "    np.linspace(0,len(times),nbins)\n",
    ")\n",
    "# print(df['agg_index'])\n",
    "# print(np.linspace(0,len(times),nbins))\n",
    "# print([[agg_time] for agg_time, df_agg in df.groupby('agg_index')])\n",
    "# print(len([[agg_time] for agg_time, df_agg in df.groupby('agg_index')])\n",
    "\n",
    "for agg_time, df_agg in df.groupby('agg_index'):\n",
    "\n",
    "    # Get first adjacency matrix\n",
    "    adj = df_agg.graph[df_agg.index[0]].get_adjacency(attribute='length')\n",
    "    \n",
    "    for g in df_agg.graph.values[1:]:\n",
    "        adj += g.get_adjacency(attribute='length')\n",
    "        \n",
    "    adj = np.array(adj.data).reshape(adj.shape)\n",
    "        \n",
    "    # Min and max in the weighted adj matrix\n",
    "    dmax = adj.max()\n",
    "    dmin = adj[adj>0].min()\n",
    "        \n",
    "    # Neighbors more often connected have low weight\n",
    "    adj[adj>0] = np.abs( adj[adj>0] - (dmin+dmax) ).astype(np.float)\n",
    "    adj = adj / n_agg_graphs\n",
    "    \n",
    "    # Create a graph from the aggregated adj matrix\n",
    "    g_agg = igraph.Graph.Adjacency((adj > 0).tolist(), mode=igraph.ADJ_UNDIRECTED)\n",
    "    g_agg.es['length'] = adj[adj.nonzero()]\n",
    "    \n",
    "    # Calculate shortest paths length\n",
    "    spl = g_agg.shortest_paths_dijkstra(weights=g_agg.es['length'])\n",
    "    distances = np.array(spl).reshape(adj.shape)\n",
    "\n",
    "    # Replace infinities with a very large distance\n",
    "    distances[np.isinf(distances)] = distances[~np.isinf(distances)].max()\n",
    "    \n",
    "    # Clustering\n",
    "    threshold = 0.3\n",
    "    fig, axs = plt.subplots(1,2,figsize=(12,4))\n",
    "    linkage = hierarchy.linkage(distances, method=\"single\")\n",
    "    clusters = hierarchy.fcluster(linkage, threshold, criterion=\"distance\")\n",
    "    dend = hierarchy.dendrogram(linkage, color_threshold=threshold, ax=axs[1])\n",
    "    order = dend['leaves']\n",
    "    distances = distances[order,:]\n",
    "    distances = distances[:,order]\n",
    "    \n",
    "    #After clustering, convert the largest distances (& not indirectly connected nodes) to nan and plot as white in the colormap\n",
    "    max_index2 = np.where(distances == np.amax(distances)) \n",
    "    distances[max_index2] = np.nan\n",
    "    current_cmap = plt.cm.get_cmap()\n",
    "    current_cmap.set_bad(color='white')\n",
    " \n",
    "    sc1 = axs[0].imshow(distances)\n",
    "#     cbar = fig.colorbar(sc1,ax=axs[0])\n",
    "    cbar = fig.colorbar(sc1, ax=axs[0],extend='neither')\n",
    "#     sc1.set_clim(vmin=0,vmax=0.9)\n",
    "    \n",
    "    cbar.set_label(r'Adjusted frame-wise distance (nm)', fontsize=14)\n",
    "    axs[0].set_title(f'Aggregate: {df_agg.time.min():.3f} to {df_agg.time.max():.3f} s')    \n",
    "    axs[1].set_xlabel(\"Node\")\n",
    "#     axs[1].set_ylabel(\"Dissimilarity\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusivity as a function of cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = pd.DataFrame([])\n",
    "\n",
    "for index in tqdm(df.index):\n",
    "    \n",
    "    g = df.graph[index]\n",
    "    \n",
    "    ccs = g.clusters()\n",
    "    \n",
    "    df_cc = pd.DataFrame([{'cluster': m, 'diffusivity': d} for (m,d) in zip(ccs.membership,g.vs['diffusivity'])])\n",
    "    sizes = df_cc.groupby('cluster').size()\n",
    "    df_cc = df_cc.groupby('cluster').agg(['mean','std'])\n",
    "    df_cc['time'] = df.time[index]\n",
    "    df_cc['nmols'] = sizes\n",
    "#     print(sum(sizes))\n",
    "#     print(df_cc)\n",
    "    \n",
    "    df_diff = pd.concat([df_diff,df_cc], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleation_time_guess = 0.5\n",
    "\n",
    "cmap = plt.cm.get_cmap('jet')\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "# sc = ax.scatter(df_diff.nmols,df_diff[('diffusivity','mean')], s=1, c=df_diff.time, cmap=cmap)\n",
    "# timemax = max(df.time)\n",
    "timemax = max(df.time)\n",
    "timemin = max(df.time)*0\n",
    "timeindex = df_diff.time[(df_diff.time >= timemin) & (df_diff.time <= timemax)].index\n",
    "sc = ax.scatter(df_diff.nmols[timeindex],df_diff[('diffusivity','mean')][timeindex],s=1, c=df_diff.time[timeindex], cmap=cmap)\n",
    "ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "plt.yscale('symlog')\n",
    "ax.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "# ax.set_ylabel(r'Avg. diffusivity ($\\times 10^{-13}~m^2/s$)', fontsize=18)\n",
    "# ax.set_xlim(0.75,20)\n",
    "# ax.set_ylim(2.5,13)\n",
    "cbar = plt.colorbar(sc,extend='neither')\n",
    "sc.set_clim(vmin=timemin,vmax=timemax)\n",
    "cbar.set_label('time (s)', fontsize=14)\n",
    "\n",
    "# Plot the average molecular diffusion constants vs the cluster size that molecules are within\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=(6,4))\n",
    "df_diff_timeindex = pd.DataFrame({\"cluster_size\": df_diff.nmols[timeindex], \"Diff_cluster_size\": df_diff[('diffusivity','mean')][timeindex]})\n",
    "Diff_cluster_size = df_diff_timeindex.groupby('cluster_size').agg(['mean','std'])\n",
    "Diff_cluster_size = Diff_cluster_size.reset_index() #Reset the dataframe to regular column\n",
    "# Linear fitting the log scale graph (check power law index)\n",
    "coef = np.polyfit(np.log10(Diff_cluster_size[\"cluster_size\"]),np.log10(Diff_cluster_size[('Diff_cluster_size','mean')]),1)\n",
    "poly1d_fn = np.poly1d(coef) # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "Diff_cluster_size_fit = [pow(10,i) for i in poly1d_fn(np.log10(Diff_cluster_size[\"cluster_size\"]))]\n",
    "plt.errorbar(Diff_cluster_size[\"cluster_size\"], Diff_cluster_size[('Diff_cluster_size','mean')], yerr = Diff_cluster_size[('Diff_cluster_size','std')], xerr = None)\n",
    "ax2.plot(Diff_cluster_size[\"cluster_size\"],Diff_cluster_size_fit,'--r',label=f'Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "ax2.legend(loc=0)\n",
    "#ax2.text(10, 0.4,)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax2.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "\n",
    "# Plot the average cluster size vs time\n",
    "fig3, ax3 = plt.subplots(1,1, figsize=(6,4))\n",
    "cluster_timeindex = pd.DataFrame({\"time\": df_diff.time[timeindex],\"cluster_size\": df_diff.nmols[timeindex]})\n",
    "cluster_time = cluster_timeindex.groupby(\"time\").agg(['mean','std'])\n",
    "cluster_time = cluster_time.reset_index() #Reset the dataframe to regular column\n",
    "# Linear fitting the log scale graph (check power law index)\n",
    "coef = np.polyfit(np.log10(cluster_time[\"time\"]),np.log10(cluster_time[('cluster_size','mean')]),1)\n",
    "poly1d_fn = np.poly1d(coef) # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "cluster_time_fit = [pow(10,i) for i in poly1d_fn(np.log10(cluster_time[\"time\"]))]\n",
    "plt.errorbar(cluster_time[\"time\"], cluster_time[('cluster_size','mean')], yerr = cluster_time[('cluster_size','std')], xerr = None)\n",
    "\n",
    "ax3.plot(cluster_time['time'],cluster_time_fit,'--r',label=f'Total: Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "\n",
    "first_select = np.where( cluster_time[\"time\"] < nucleation_time_guess)[0]\n",
    "coef = np.polyfit(np.log10(cluster_time[\"time\"][first_select]),np.log10(cluster_time[('cluster_size','mean')][first_select]),1)\n",
    "poly1d_fn = np.poly1d(coef)\n",
    "cluster_time_fit = [pow(10,i) for i in poly1d_fn(np.log10(cluster_time[\"time\"]))]\n",
    "ax3.plot(cluster_time['time'],cluster_time_fit,'--g',label=f'First: Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "\n",
    "\n",
    "ax3.legend(loc=0)\n",
    "#ax3.text(0.05, 10, f'Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_xlabel('time / s', fontsize=18)\n",
    "ax3.set_ylabel(r'Avg. cluster size (#molecules)', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.865108,
     "end_time": "2023-04-01T23:38:56.449028",
     "exception": false,
     "start_time": "2023-04-01T23:38:54.583920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Added Cell 2: Replot the cluster diffusivity with cluster size by combining all time information and use single color for each dots but also add mean(not median) and std values with dots on the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.860739,
     "end_time": "2023-04-01T23:39:00.171900",
     "exception": false,
     "start_time": "2023-04-01T23:38:58.311161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Combined 1st and 2nd figure of Deff vs. cluster size without time information as colormap\n",
    "cmap = plt.cm.get_cmap('jet')\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "timemax = max(df.time)\n",
    "timemin = max(df.time)*0\n",
    "timeindex = df_diff.time[(df_diff.time >= timemin) & (df_diff.time <= timemax)].index\n",
    "sc = ax.scatter(df_diff.nmols[timeindex],df_diff[('diffusivity','mean')][timeindex], s=1, c='grey')\n",
    "\n",
    "# Plot the average molecular diffusion constants vs the cluster size that molecules are within\n",
    "df_diff_timeindex = pd.DataFrame({\"cluster_size\": df_diff.nmols[timeindex], \"Diff_cluster_size\": df_diff[('diffusivity','mean')][timeindex]})\n",
    "Diff_cluster_size = df_diff_timeindex.groupby('cluster_size').agg(['mean','std'])\n",
    "Diff_cluster_size = Diff_cluster_size.reset_index() #Reset the dataframe to regular column\n",
    "\n",
    "plt.errorbar(Diff_cluster_size[\"cluster_size\"], Diff_cluster_size[('Diff_cluster_size','mean')], yerr = Diff_cluster_size[('Diff_cluster_size','std')], xerr = None)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "ax.set_ylim(1e-5,10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
